# ğŸ¯ Live Data Capture & Training Guide (Optimized)

## ğŸš€ NEW APPROACH: 2-Hour Live Capture

Instead of fetching 30 days of historical data (which is computationally expensive), this **optimized approach** captures **LIVE streaming data for 2 hours** and trains ML models on real-time patterns.

---

## âœ¨ Why This Approach is Better

### âŒ Old Approach (30-Day Historical):
- **Data volume**: ~388,800 candles (30 days Ã— 1440 min Ã— 9 streams)
- **API calls**: ~600+ requests to fetch all data
- **Time**: 10-15 minutes just to fetch
- **Storage**: ~27 MB of CSV files
- **Computation**: Heavy processing of 130,000+ records
- **Issue**: Rate limits, network delays, cache management

### âœ… New Approach (2-Hour Live Capture):
- **Data volume**: ~1,080 updates (2 hours Ã— 60 min Ã— 9 streams)
- **API calls**: 0 (streaming WebSocket data)
- **Time**: Exactly 2 hours real-time
- **Storage**: ~2 MB total
- **Computation**: Lightweight, processes as data arrives
- **Benefits**: Fresh data, no rate limits, captures current market conditions

---

## ğŸ“Š Expected Data Volume (2 Hours)

### Price Updates:
```
Duration:        2 hours
Exchanges:       3 (Coinbase, Binance, Bitstamp)
Symbols:         3 (BTC-USD, ETH-USD, SOL-USD)
Update frequency: ~1 update per minute per stream

Total updates:   2 hours Ã— 60 min Ã— 9 streams = ~1,080 price updates
```

### Arbitrage Opportunities:
```
Expected rate:   ~20 opportunities per hour
Total for 2h:    ~40 profitable arbitrage opportunities
```

### Memory Usage:
```
Raw price data:   ~1 MB
Opportunities:    ~50 KB
ML models:        ~2 MB
Total:            ~3 MB (very lightweight!)
```

---

## ğŸ¯ Quick Start

### Run 2-Hour Capture (Default):
```bash
python train_live_capture.py
```

This will:
1. âœ… Connect to all 3 exchanges
2. âœ… Stream live prices for 2 hours
3. âœ… Detect arbitrage opportunities in real-time
4. âœ… Save captured data to `captured_data/`
5. âœ… Train ML models on live data
6. âœ… Save models to `models/`

---

## âš™ï¸ Custom Capture Duration

### 30-Minute Quick Test:
```bash
python train_live_capture.py --hours 0.5
```

### 1-Hour Capture:
```bash
python train_live_capture.py --hours 1
```

### 4-Hour Extended Capture:
```bash
python train_live_capture.py --hours 4
```

### Overnight Capture (8 hours):
```bash
python train_live_capture.py --hours 8
```

**Recommendation**: 2 hours is optimal for training while keeping computation light

---

## ğŸ“ˆ What Happens During Capture

### Minute 0-5: Initialization
```
ğŸš€ LIVE DATA CAPTURE - 2 HOURS
============================================================
Start: 2025-11-01 18:00:00
End:   2025-11-01 20:00:00
Exchanges: Coinbase, Binance, Bitstamp
Symbols: BTC-USD, ETH-USD, SOL-USD
============================================================

âœ“ Connected to Bitstamp
âœ“ Connected to Coinbase
âœ“ Connected to Binance

Streaming data...
```

### Every 5 Minutes: Progress Update
```
ğŸ“Š Progress Update:
  Elapsed: 0.5h / 2h (25%)
  Remaining: 1.5h
  Price updates: 270
  Opportunities: 10
  Avg opportunities/hour: 20.0
```

### After 2 Hours: Completion
```
âœ… Capture complete! 2 hours elapsed

ğŸ’¾ SAVING CAPTURED DATA
============================================================
âœ“ Saved 1,080 price records to captured_data/prices_20251101_200000.csv
âœ“ Saved 40 opportunities to captured_data/opportunities_20251101_200000.csv
============================================================

ğŸ“ˆ CAPTURE SUMMARY
============================================================
Duration: 2.00 hours
Price updates: 1,080
Opportunities detected: 40

Opportunity Statistics:
  Average profit: 0.673%
  Max profit: 1.245%
  Min profit: 0.512%
  Opportunities/hour: 20.0

Top Exchange Pairs:
  Coinbase->Binance:BTC-USD: 12 opportunities
  Binance->Bitstamp:ETH-USD: 9 opportunities
  Bitstamp->Coinbase:SOL-USD: 8 opportunities
============================================================

ğŸ¤– TRAINING ML MODELS
============================================================
1ï¸âƒ£ Training Spread Predictor on 1,080 records...
âœ“ Spread predictor saved to models/spread_predictor_live.pkl

2ï¸âƒ£ Training Opportunity Classifier on 40 opportunities...
âœ“ Opportunity classifier saved to models/opportunity_classifier_live.pkl
============================================================

ğŸ‰ TRAINING COMPLETE!
============================================================
Models trained on live data and saved to models/
```

---

## ğŸ“‚ Output Files

### After capture, you'll have:

```
crypto_arbitrage/
â”œâ”€â”€ captured_data/              # Raw captured data
â”‚   â”œâ”€â”€ prices_20251101_200000.csv
â”‚   â””â”€â”€ opportunities_20251101_200000.csv
â”‚
â”œâ”€â”€ models/                     # Trained models
â”‚   â”œâ”€â”€ spread_predictor_live.pkl
â”‚   â””â”€â”€ opportunity_classifier_live.pkl
â”‚
â””â”€â”€ [other project files...]
```

### Data Files:

**prices_*.csv**:
```csv
exchange,price,timestamp,bid,ask,volume
Coinbase,43250.50,2025-11-01 18:00:01,43249.00,43252.00,1.24
Binance,43248.20,2025-11-01 18:00:01,43247.00,43249.40,2.15
...
```

**opportunities_*.csv**:
```csv
buy_exchange,sell_exchange,symbol,buy_price,sell_price,spread_pct,profit_after_fees,timestamp
Binance,Coinbase,BTC-USD,43248.20,43250.50,0.053,0.673,2025-11-01 18:05:23
...
```

---

## ğŸ® Usage Tips

### 1. Run While You Work
```bash
# Start capture in background
python train_live_capture.py --hours 2

# Continue working on other things
# Come back in 2 hours to trained models!
```

### 2. Interrupt Early if Needed
Press **Ctrl+C** at any time to stop early:
```
âš ï¸ Received interrupt signal, stopping capture...
âœ… Saving data captured so far...
```

The script will still save data and train models on whatever was captured.

### 3. Multiple Captures
Run multiple times to gather different market conditions:
```bash
# Morning capture
python train_live_capture.py --hours 2  # 8am-10am

# Afternoon capture
python train_live_capture.py --hours 2  # 2pm-4pm

# Evening capture
python train_live_capture.py --hours 2  # 8pm-10pm
```

Different times = different volatility = better ML training diversity

---

## ğŸ’¡ Performance Comparison

| Metric | 30-Day Historical | 2-Hour Live Capture |
|--------|------------------|---------------------|
| **Data Points** | ~388,800 | ~1,080 |
| **Fetch Time** | 10-15 min | 0 (streaming) |
| **Total Time** | 15-20 min | 2 hours |
| **Storage** | 27 MB | 2 MB |
| **API Calls** | ~600 | 0 |
| **Rate Limits** | Risk of hitting | None |
| **Computation** | Heavy (130k records) | Light (1k records) |
| **Data Freshness** | Up to 30 days old | Real-time |
| **ML Training Time** | 3-5 min | 10-30 sec |

---

## ğŸ§® Computational Requirements

### Memory Usage (2-Hour Capture):
```
Price buffer:       ~1 MB
Opportunity list:   ~50 KB
WebSocket overhead: ~5 MB
ML training:        ~10 MB peak
Total:              ~16 MB
```

**Result**: Can run on ANY machine, even low-spec laptops!

### CPU Usage:
```
Data streaming:     <5% CPU
Arbitrage detection: <10% CPU
ML training:        20-30% CPU (only at end)
```

**Result**: Won't slow down your system during capture

---

## ğŸ¯ Training Quality

### 2 Hours is Sufficient Because:

1. **Market Patterns Repeat**: Crypto markets have hourly cycles, 2 hours captures multiple cycles

2. **High Frequency**: With 9 streams updating per minute, you get 1,080 data points

3. **Diverse Conditions**: 2 hours spans different volatility levels

4. **Recent Data**: ML models learn current market behavior, not outdated patterns

5. **Opportunity Density**: ~40 opportunities is enough to train classifier

---

## ğŸ“Š Expected ML Performance

### With 2 Hours of Data:

**Spread Predictor**:
- Training samples: ~1,080
- Features: ~30 per record
- Expected RÂ² score: 0.65-0.75
- Prediction accuracy: Â±0.2% spread

**Opportunity Classifier**:
- Training samples: ~40 opportunities
- Accuracy: 70-80%
- Precision: 75-85%

These metrics are **sufficient** for real-time arbitrage detection!

---

## ğŸ”„ Integration with Main System

After capture and training, use the models:

```python
# main.py will automatically load them
import joblib

# Try to load live-trained models first
try:
    predictor = joblib.load('models/spread_predictor_live.pkl')
    classifier = joblib.load('models/opportunity_classifier_live.pkl')
    logger.success("Loaded live-trained models!")
except:
    # Fallback to untrained
    predictor = SpreadPredictor()
    classifier = OpportunityClassifier()
```

---

## ğŸ› ï¸ Troubleshooting

### Issue: "Capture taking too long"
**Solution**: Use shorter duration
```bash
python train_live_capture.py --hours 1
```

### Issue: "Not enough opportunities captured"
**Cause**: Low market volatility during capture period

**Solution**: Either:
1. Lower profit threshold in `config.py`:
   ```python
   MIN_PROFIT_THRESHOLD = 0.3  # From 0.5 to 0.3
   ```
2. Capture during peak trading hours (2pm-6pm UTC)

### Issue: "Want to pause/resume capture"
**Current limitation**: Not supported. If interrupted, data is saved but capture won't resume.

**Workaround**: Run multiple shorter captures:
```bash
python train_live_capture.py --hours 1
# Wait a bit
python train_live_capture.py --hours 1
```

---

## ğŸ“ˆ Best Practices

### 1. Choose Peak Hours
Capture during high-volatility periods:
- **Best**: 2pm-6pm UTC (US market hours)
- **Good**: 8am-12pm UTC (European market open)
- **Avoid**: Midnight-6am UTC (low volume)

### 2. Monitor Progress
Keep terminal visible to watch:
- Price update frequency
- Opportunity detection rate
- Progress percentage

### 3. Validate After Training
```python
import joblib

# Load and test
predictor = joblib.load('models/spread_predictor_live.pkl')
print(f"Model trained: {predictor.is_trained}")
print(f"Features: {len(predictor.feature_names)}")
```

---

## ğŸ‰ Summary

### Old Approach (Historical):
- âŒ Fetch 30 days = 10-15 min
- âŒ Process 388k records = heavy computation
- âŒ Risk of rate limits
- âŒ 27 MB storage

### New Approach (Live 2h):
- âœ… Capture 2 hours = real-time streaming
- âœ… Process 1k records = lightweight
- âœ… No rate limits
- âœ… 2 MB storage
- âœ… **Better for hackathon demo!**

---

## ğŸš€ Quick Command Reference

```bash
# Standard 2-hour capture
python train_live_capture.py

# Quick 30-minute test
python train_live_capture.py --hours 0.5

# Extended 4-hour capture
python train_live_capture.py --hours 4

# After training, run main system
python main.py
```

---

## ğŸ† Why This Wins at Hackathon

1. **Real-time focus**: Shows live data processing, not historical analysis
2. **Efficient**: Low computation, runs anywhere
3. **Demo-friendly**: Can start capture at beginning of hackathon, have models ready
4. **Scalable**: Same approach works for 1 hour or 24 hours
5. **Modern**: Streaming-first architecture (what judges want to see!)

---

**Ready to capture live data? Run:**

```bash
python train_live_capture.py
```

**Then sit back for 2 hours and let it work! â˜•**
